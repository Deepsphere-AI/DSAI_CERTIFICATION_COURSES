{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Data_Preparation_with_Python.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## **Copyright Notice**"],"metadata":{"id":"BQXfEZwTs9wo"}},{"cell_type":"markdown","source":["Local and international copyright laws protect this material. Repurposing and reproducing this\n","material from DeepSphere.AI violates the law"],"metadata":{"id":"I1fvbR04S0KC"}},{"cell_type":"markdown","source":["## **Disclaimer**"],"metadata":{"id":"eDXMyMwFS_ii"}},{"cell_type":"markdown","source":["We are providing this code block strictly for learning and researching, this is not a\n","production-ready code. We have no liability on this particular code under any circumstances;\n","users should use this code at their own risk. All software, hardware and other products that\n","are referenced in these materials belong to the respective vendor who developed or who owns\n","this product"],"metadata":{"id":"kshyuptfS_tM"}},{"cell_type":"markdown","source":["## **Data Preparation in Python**"],"metadata":{"id":"85Tpdu5zS0bs"}},{"cell_type":"markdown","source":["Data comes in different formats from various sources. Data preparation integrates the data for AI model training and testing. Instead of the traditional ETL process, we use machine learning techniques to prepare the training and test data sets. To schedule training and test data for the model, a formal ETL process may not be an option as data changes when there is a change in the business process. Thatâ€™s why we use the machine learning technique for data preparation to learn and understand the changes in the data and its pattern. "],"metadata":{"id":"ypDmqByxtBbF"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from google.colab import drive\n","\n","class Data_Preparation_in_python:\n","  def __init__(self):\n","    return None\n","\n","## Load Data from the Google Drive\n","\n","  def Loading_Data(self):\n","    vAR_series = pd.Series([1,2,3,4,None,np.nan])\n","    #print(vAR_series)\n","\n","## Read Data from the Google Drive\n","\n","  def read_data_from_drive(self):\n","    drive.mount('/content/drive')\n","    self.vAR_Data_Path = '/content/drive/MyDrive/Data Engineering/Intermediate/Unit-6/Source Data/Data.xlsx'\n","\n","## Collect the Data into a DataFrame Object\n","\n","  def collect_data(self):\n","    self.data_collected = pd.read_excel(self.vAR_Data_Path)\n","    #return self.data_collected.head()\n","\n","## Find the missing values\n","\n","  def Missing_values(self):\n","    #print(self.data_collected.isnull())\n","    self.dropped = self.data_collected.dropna()\n","    #print(self.dropped)\n","\n","## Create a New DataFrame\n","\n","  def new_dataframe(self):\n","    self.new_df = pd.DataFrame([[50,60,70,np.nan],[80,90,np.nan,np.nan],[100,np.nan,np.nan,np.nan],[np.nan,np.nan,np.nan,np.nan]])\n","    self.new_df.dropna(how='all')\n","    self.new_df.dropna(axis=1, how='all')\n","    self.new_df.dropna(axis=1, thresh=2)\n","    #print(self.new_df)                           \n","\n","## Fill the null value with the mean value\n","\n","  def fill_nullvalues(self):\n","    self.new_df.fillna(self.new_df.mean(),inplace=True)\n","    #print(self.new_df)\n","\n","## Replace the negative value with the specified value\n","\n","  def replace_function(self):\n","    self.newseries = pd.Series([100,200,-50,40,50,-50,70,80,-50])\n","    self.newseries.replace(-50,self.newseries.mean())\n","    self.newseries1 = pd.Series([300,400,-50,40,50,-50,70,80,-50])\n","    self.Series_Concat = pd.concat([self.newseries,self.newseries1],ignore_index=True) \n","    #print(self.Series_Concat)\n","\n","## Apply the Map Function\n","\n","  def map_function(self):\n","     self.data_number = pd.DataFrame({'Marks': ['50','74','62','84','56','65'],'Grade': ['B','A','B','A+','B','B']})\n","     self.yes_no = {'50': 'yes','74': 'yes'}\n","     self.data_number['Yes/No'] = self.data_number['Marks'].map(self.yes_no)\n","     print(self.yes_no) \n","\n","## One-Hot Encoding\n","  def One_hot_encoding(self):\n","    self.data = pd.Series(list('JOHNJOHNMARKMARK'))\n","    self.onehot_encoding = pd.get_dummies(self.data)\n","    print(self.onehot_encoding)\n","\n","vAR_obj = Data_Preparation_in_python()\n","vAR_obj.Loading_Data()\n","vAR_obj.read_data_from_drive()\n","vAR_obj.collect_data()\n","vAR_obj.Missing_values()\n","vAR_obj.new_dataframe()\n","vAR_obj.fill_nullvalues()\n","vAR_obj.replace_function()\n","vAR_obj.map_function()\n","vAR_obj.One_hot_encoding()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ITpZFCHBs_5Z","outputId":"7c458f8f-027e-4654-a0c1-7a0c479bf96a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","{'50': 'yes', '74': 'yes'}\n","    A  H  J  K  M  N  O  R\n","0   0  0  1  0  0  0  0  0\n","1   0  0  0  0  0  0  1  0\n","2   0  1  0  0  0  0  0  0\n","3   0  0  0  0  0  1  0  0\n","4   0  0  1  0  0  0  0  0\n","5   0  0  0  0  0  0  1  0\n","6   0  1  0  0  0  0  0  0\n","7   0  0  0  0  0  1  0  0\n","8   0  0  0  0  1  0  0  0\n","9   1  0  0  0  0  0  0  0\n","10  0  0  0  0  0  0  0  1\n","11  0  0  0  1  0  0  0  0\n","12  0  0  0  0  1  0  0  0\n","13  1  0  0  0  0  0  0  0\n","14  0  0  0  0  0  0  0  1\n","15  0  0  0  1  0  0  0  0\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"Js0N-dN9t4Z5"},"execution_count":null,"outputs":[]}]}